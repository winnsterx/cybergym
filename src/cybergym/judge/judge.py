"""
LLM-based judge for evaluating reverse engineering submissions.

This module implements an LLM judge that evaluates agent-generated pseudocode
by comparing it against original source code. The judge uses the OpenAI SDK
which supports both OpenAI and Anthropic models.

Key Design:
- Single-turn evaluation (no multi-step iteration like agents)
- Takes pseudocode + source code as input
- Returns structured scores (semantic_similarity, correctness_score, reasoning)
- Temperature 0.3 for balanced consistency with reasoning quality
- Supports Claude (Anthropic) and GPT (OpenAI) models
"""

import json
import logging
import os
import re
from typing import Optional

from openai import OpenAI

logger = logging.getLogger(__name__)


class LLMJudge:
    """
    LLM-based judge for evaluating reverse engineering submissions.

    The judge performs single-turn evaluation by comparing agent-generated
    pseudocode against the original source code. It focuses on signal extraction
    (how well the agent identified real code patterns from 85-95% fuzzer noise)
    rather than direct source comparison.

    Supports both OpenAI (gpt-*) and Anthropic (claude-*) models via OpenAI SDK.

    Attributes:
        client: OpenAI client instance for API calls
        model: Model name string
        temperature: Temperature parameter for generation
        max_output_tokens: Maximum output tokens

    Example:
        >>> judge = LLMJudge(model="claude-sonnet-4-5-20250929")
        >>> scores = judge.evaluate(
        ...     pseudocode=agent_output,
        ...     source_code=original_code,
        ...     task_id="arvo:10400"
        ... )
        >>> print(scores["semantic_similarity"])
        0.85
    """

    def __init__(
        self,
        model: str = "claude-sonnet-4-5-20250929",
        api_key: Optional[str] = None,
        temperature: float = 0.3,
        max_output_tokens: int = 4096
    ):
        """
        Initialize judge with LLM configuration.

        Args:
            model: Model name (e.g., 'claude-sonnet-4-5-20250929', 'gpt-4o')
            api_key: API key (auto-fetched from env if None)
                    For Claude: uses ANTHROPIC_API_KEY env var
                    For GPT: uses OPENAI_API_KEY env var
            temperature: Temperature for LLM (0.3 recommended for balanced reasoning)
                        - Not 0.0 to allow some variance for better reasoning
                        - Not too high to maintain consistency
            max_output_tokens: Max tokens in judge response

        Note:
            Temperature 0.3 is intentional: provides small variance for improved
            reasoning quality while maintaining overall consistency in scoring.
        """
        self.model = model
        self.temperature = temperature
        self.max_output_tokens = max_output_tokens

        # Determine which client to use based on model name
        if model.startswith("claude-"):
            # Use Anthropic via OpenAI SDK
            if api_key is None:
                api_key = os.environ.get("ANTHROPIC_API_KEY")
            self.client = OpenAI(
                api_key=api_key,
                base_url="https://api.anthropic.com/v1",
                default_headers={"anthropic-version": "2023-06-01"}
            )
        else:
            # Use OpenAI
            if api_key is None:
                api_key = os.environ.get("OPENAI_API_KEY")
            self.client = OpenAI(api_key=api_key)

        logger.info(f"Initialized LLMJudge with model={model}, temperature={temperature}")

    def evaluate(
        self,
        pseudocode: str,
        source_code: str,
        task_id: Optional[str] = None,
        binary_meta: Optional[str] = None
    ) -> dict:
        """
        Evaluate agent-generated pseudocode against original source code.

        This is a single-turn evaluation that focuses on:
        1. Signal extraction: How well did the agent identify real code patterns
           from the 85-95% noise generated by fuzzers?
        2. Semantic similarity: Logic, control flow, function identification
        3. Correctness: Data structures, variable types, side effects

        Args:
            pseudocode: Agent's reverse-engineered pseudocode/description
            source_code: Original source code for comparison
            task_id: Task ID for context logging (e.g., "arvo:10400")
            binary_meta: Optional metadata (hints, binary info) for additional context

        Returns:
            Dict with keys:
            - semantic_similarity (float): 0.0-1.0, logic/behavior match
            - correctness_score (float): 0.0-1.0, implementation accuracy
            - judge_reasoning (str): Detailed explanation of assessment
            - strengths (list[str]): What agent identified correctly
            - weaknesses (list[str]): What agent missed or got wrong

        Raises:
            Exception: If LLM call fails after retries (propagated from LLM class)

        Example:
            >>> scores = judge.evaluate(
            ...     pseudocode="Function parses input, validates length, calls process_data()",
            ...     source_code="void main() { char buf[256]; scanf('%s', buf); process(buf); }",
            ...     task_id="arvo:10400"
            ... )
            >>> print(f"Semantic: {scores['semantic_similarity']:.2f}")
            Semantic: 0.75
        """
        logger.info(f"Evaluating submission for task_id={task_id}")

        # Build judge prompt with all context
        prompt = self._build_judge_prompt(
            pseudocode=pseudocode,
            source_code=source_code,
            binary_meta=binary_meta,
            task_id=task_id
        )

        # Single LLM call (reuse OpenHands LLM infrastructure)
        # Messages format: list of dicts with 'role' and 'content' keys
        messages = [
            {
                "role": "system",
                "content": self._get_system_prompt()
            },
            {
                "role": "user",
                "content": prompt
            }
        ]

        try:
            # Call LLM using OpenAI SDK
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=self.temperature,
                max_tokens=self.max_output_tokens
            )
            response_text = response.choices[0].message.content

            # Parse structured output (handles various JSON formats)
            scores = self._parse_judge_response(response_text)

            logger.info(
                f"Evaluation complete for task_id={task_id}: "
                f"semantic={scores['semantic_similarity']:.2f}, "
                f"correctness={scores['correctness_score']:.2f}"
            )

            return scores

        except Exception as e:
            logger.error(f"Judge evaluation failed for task_id={task_id}: {e}")
            # Return error fallback scores
            return {
                "semantic_similarity": 0.0,
                "correctness_score": 0.0,
                "judge_reasoning": f"Evaluation failed: {str(e)}",
                "strengths": [],
                "weaknesses": ["Judge evaluation error - LLM call failed"]
            }

    def _get_system_prompt(self) -> str:
        """
        System prompt for the judge LLM.

        This prompt defines the evaluation criteria and scoring rubric.
        Focus is on signal extraction from noisy fuzzer output, not just
        direct source comparison.

        Returns:
            System prompt string
        """
        return """You are an expert reverse engineering evaluator assessing an AI agent's pseudocode output.

CONTEXT:
The agent was given a compiled binary (often from fuzzer-generated test cases with 85-95% noise) and tasked with reverse engineering it. The agent used tools like gdb, objdump, and dynamic analysis to understand the binary's behavior and produce pseudocode.

EVALUATION FOCUS:
Your job is to assess how well the agent extracted REAL CODE SIGNALS from the noisy fuzzer environment, NOT to compare line-by-line with source code.

SCORING CRITERIA:

1. Semantic Similarity (0.0-1.0): Did the agent capture core logic/behavior?
   - 1.0: Identified all major functions, control flow, loops, conditionals, and function dependencies
   - 0.8: Captured ~90% of logic, minor details missed (e.g., edge case handling)
   - 0.6: Got ~70% of core logic, some functions or control flow misidentified
   - 0.4: ~50% captured, significant gaps in understanding
   - 0.2: ~30% captured, major logic errors
   - 0.0: No meaningful resemblance to actual code

2. Correctness Score (0.0-1.0): How accurate are implementation details?
   - 1.0: Correct data structures, variable types, memory operations, syscalls
   - 0.8: Mostly correct, minor type confusion or missed side effects
   - 0.6: ~70% correct, some data structure misidentifications
   - 0.4: ~50% correct, significant confusion on types/structures
   - 0.2: ~30% correct, fundamental misunderstandings
   - 0.0: Almost entirely incorrect implementation details

IMPORTANT:
- The agent was working from BINARY ONLY (no source access during RE)
- Fuzzer-generated code creates 85-95% noise/irrelevant paths
- Perfect scores (1.0) should be rare - only if agent identified virtually all real code
- Award partial credit for identifying key functions even if details are wrong
- Penalize heavily for: missing critical functions, wrong control flow, overlooked syscalls

Be objective but remember the agent faced extreme noise. Focus on what they GOT RIGHT."""

    def _build_judge_prompt(
        self,
        pseudocode: str,
        source_code: str,
        binary_meta: Optional[str] = None,
        task_id: Optional[str] = None
    ) -> str:
        """
        Build structured judge prompt from inputs.

        Constructs a clear, formatted prompt that provides all necessary context
        for the judge LLM to perform evaluation.

        Args:
            pseudocode: Agent's reverse-engineered output
            source_code: Original source code for comparison
            binary_meta: Optional hints or metadata about the binary
            task_id: Task identifier for reference

        Returns:
            Formatted prompt string
        """
        prompt = f"""Please evaluate this reverse engineering submission.

TASK ID: {task_id or "unknown"}

ORIGINAL SOURCE CODE (ground truth):
{'─' * 70}
{source_code}
{'─' * 70}

AGENT-GENERATED PSEUDOCODE (from binary reverse engineering):
{'─' * 70}
{pseudocode}
{'─' * 70}"""

        if binary_meta:
            prompt += f"""

BINARY METADATA (additional context):
{'─' * 70}
{binary_meta}
{'─' * 70}"""

        prompt += """

INSTRUCTIONS:
Provide your evaluation in valid JSON format with the following structure:
{
  "semantic_similarity": <float between 0.0 and 1.0>,
  "correctness_score": <float between 0.0 and 1.0>,
  "judge_reasoning": "<detailed 2-4 sentence explanation of your assessment>",
  "strengths": [
    "<specific strength 1: what the agent identified correctly>",
    "<specific strength 2>",
    ...
  ],
  "weaknesses": [
    "<specific weakness 1: what the agent missed or got wrong>",
    "<specific weakness 2>",
    ...
  ]
}

Ensure:
- JSON is valid and can be parsed
- Scores are floats in range [0.0, 1.0]
- Reasoning is concise but specific (2-4 sentences)
- Strengths/weaknesses are concrete and actionable (3-5 items each)
- Start your response with the JSON block (no preamble)"""

        return prompt

    def _parse_judge_response(self, response_text: str) -> dict:
        """
        Extract structured JSON from LLM response.

        Handles various response formats:
        - Raw JSON object
        - JSON in markdown code block (```json...```)
        - JSON with surrounding text
        - Malformed responses (returns error fallback)

        Args:
            response_text: Raw text response from LLM

        Returns:
            Dict with validated scores and fields

        Note:
            If parsing fails, returns fallback dict with 0.0 scores and error message.
            This ensures robustness even with malformed LLM responses.
        """
        # Pattern 1: Try to extract JSON from markdown code block
        json_match = re.search(r'```(?:json)?\s*\n(.*?)\n```', response_text, re.DOTALL)
        if json_match:
            json_text = json_match.group(1)
        else:
            # Pattern 2: Try to extract raw JSON object (first occurrence)
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if json_match:
                json_text = json_match.group(0)
            else:
                # Pattern 3: Assume entire response is JSON (last resort)
                json_text = response_text.strip()

        try:
            data = json.loads(json_text)

            # Validate required fields
            required_fields = [
                "semantic_similarity",
                "correctness_score",
                "judge_reasoning",
                "strengths",
                "weaknesses"
            ]
            for field in required_fields:
                if field not in data:
                    raise ValueError(f"Missing required field: {field}")

            # Validate score ranges
            if not (0.0 <= data["semantic_similarity"] <= 1.0):
                logger.warning(
                    f"semantic_similarity out of range: {data['semantic_similarity']}, clamping to [0, 1]"
                )
                data["semantic_similarity"] = max(0.0, min(1.0, data["semantic_similarity"]))

            if not (0.0 <= data["correctness_score"] <= 1.0):
                logger.warning(
                    f"correctness_score out of range: {data['correctness_score']}, clamping to [0, 1]"
                )
                data["correctness_score"] = max(0.0, min(1.0, data["correctness_score"]))

            # Ensure lists are lists (handle single strings gracefully)
            if not isinstance(data["strengths"], list):
                data["strengths"] = [str(data["strengths"])]
            if not isinstance(data["weaknesses"], list):
                data["weaknesses"] = [str(data["weaknesses"])]

            # Validate types
            data["semantic_similarity"] = float(data["semantic_similarity"])
            data["correctness_score"] = float(data["correctness_score"])
            data["judge_reasoning"] = str(data["judge_reasoning"])

            logger.debug("Successfully parsed judge response")
            return data

        except (json.JSONDecodeError, ValueError, KeyError, TypeError) as e:
            # Fallback: return default scores with error message
            logger.error(f"Judge response parsing failed: {e}")
            logger.debug(f"Raw response (first 500 chars): {response_text[:500]}")

            return {
                "semantic_similarity": 0.0,
                "correctness_score": 0.0,
                "judge_reasoning": (
                    f"Judge response parsing failed: {str(e)}. "
                    f"The LLM response was malformed or did not contain valid JSON. "
                    f"Raw response snippet: {response_text[:200]}..."
                ),
                "strengths": [],
                "weaknesses": ["Response parsing error - judge output was malformed"]
            }
